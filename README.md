# ğŸ–ï¸ Sign Language to Speech â€“ MediaPipe & LSTM

This project is a **real-time sign language recognition system** that detects hand and body movements from a camera, classifies them using a deep learning model.

https://github.com/user-attachments/assets/793e85af-dbbd-4590-b02a-3ac968036f37

## ğŸš€ Project Overview

The system works in real time and follows this pipeline:

```
Camera â†’ MediaPipe Holistic â†’ Landmark Extraction
       â†’ LSTM Model â†’ Sign Classification
```

Instead of using single images, each sign is treated as a **sequence of frames**, allowing the model to understand motion and temporal patterns.

---

## ğŸ§  How It Works

### 1ï¸âƒ£ Landmark Extraction (MediaPipe)

* Uses **MediaPipe Holistic**
* Extracts:

  * Right hand landmarks (21 points)
  * Left hand landmarks (21 points)
  * Body pose landmarks (33 points)
* Each frame is converted into a numerical feature vector

### 2ï¸âƒ£ Sequence-Based Learning (LSTM)

* Each sign is recorded as **30 consecutive frames**
* These sequences are fed into an **LSTM (Long Short-Term Memory)** neural network
* LSTM learns the temporal structure of gestures rather than static poses

### 3ï¸âƒ£ Real-Time Inference

* Live camera feed is processed frame by frame
* The last 30 frames are continuously evaluated
* The model predicts the most likely sign

## ğŸ§© Supported Signs (Example)

```text
MERHABA
EVET
HAYIR
```

## ğŸ› ï¸ Technologies Used

* **Python**
* **MediaPipe (Holistic)**
* **TensorFlow / Keras**
* **LSTM Neural Networks**
* **OpenCV**
* **NumPy***
* **scikit-learn**


---

## ğŸ“Š Data Format

Each recorded sample is saved as a NumPy file:

```python
(30, 258)
```

* `30` â†’ number of frames (sequence length)
* `258` â†’ feature vector per frame (hands + pose landmarks)

---

## â–¶ï¸ How to Run

### 1ï¸âƒ£ Create & activate virtual environment

```bash
python -m venv .venv
.venv\Scripts\activate
```

### 2ï¸âƒ£ Install dependencies

```bash
pip install opencv-python mediapipe tensorflow numpy scikit-learn
```

### 3ï¸âƒ£ Collect data

```bash
python collect_data.py
```

### 4ï¸âƒ£ Train the model

```bash
python train_lstm.py
```

### 5ï¸âƒ£ Run real-time inference

```bash
python realtime_inference.py
```

---

## ğŸ¯ Key Learnings

* Temporal data is critical for gesture recognition
* LSTM models outperform single-frame classifiers for motion-based tasks
* MediaPipe provides a powerful and efficient way to extract body landmarks


## ğŸ“Œ Disclaimer

This project is for **educational and experimental purposes**.
It is not intended to replace professional sign language interpreters.




